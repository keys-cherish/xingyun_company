"""Daily settlement scheduler using APScheduler."""

from __future__ import annotations

import datetime as dt
import gzip
import json
import logging
from decimal import Decimal
from pathlib import Path
from zoneinfo import ZoneInfo

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from sqlalchemy import select, text

from config import settings
from db.engine import async_session, engine
from cache.redis_client import add_stream_event
from services.settlement_service import format_daily_report, settle_all
from utils.timezone import BJ_TZ, format_bj_now

logger = logging.getLogger(__name__)

_scheduler: AsyncIOScheduler | None = None
_bot_ref = None  # will be set at startup
_BACKUP_DIR = Path(".")


def set_bot(bot):
    global _bot_ref
    _bot_ref = bot


def _json_safe(value):
    if isinstance(value, (dt.datetime, dt.date, dt.time)):
        return value.isoformat()
    if isinstance(value, Decimal):
        return int(value) if value == value.to_integral_value() else float(value)
    return value


def _write_backup_file(path: Path, payload: dict):
    with gzip.open(path, "wt", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False)


def _rotate_old_backups(keep_files: int):
    if keep_files <= 0:
        return
    files = sorted(_BACKUP_DIR.glob("my_company_backup_*.json.gz"))
    overflow = len(files) - keep_files
    if overflow <= 0:
        return
    for old_file in files[:overflow]:
        old_file.unlink(missing_ok=True)


async def _create_db_backup() -> tuple[Path, dict[str, int]]:
    from db.models import Base

    _BACKUP_DIR.mkdir(parents=True, exist_ok=True)
    now_bj = dt.datetime.now(BJ_TZ)
    file_path = _BACKUP_DIR / f"my_company_backup_{now_bj.strftime('%Y%m%dT%H%M%S%z')}.json.gz"

    table_data: dict[str, list[dict]] = {}
    table_counts: dict[str, int] = {}

    async with engine.connect() as conn:
        for table in Base.metadata.sorted_tables:
            result = await conn.execute(text(f'SELECT * FROM "{table.name}"'))
            rows = []
            for row in result.mappings().all():
                rows.append({k: _json_safe(v) for k, v in row.items()})
            table_data[table.name] = rows
            table_counts[table.name] = len(rows)

    payload = {
        "project": "my_company",
        "created_at_bj": now_bj.isoformat(),
        "tables": table_data,
    }
    _write_backup_file(file_path, payload)
    _rotate_old_backups(settings.backup_keep_files)
    return file_path, table_counts


async def _notify_backup_status(text_msg: str):
    if not _bot_ref or not settings.backup_notify_super_admin:
        return
    admin_ids = settings.super_admin_tg_id_set
    if not admin_ids:
        return
    for admin_tg_id in admin_ids:
        try:
            await _bot_ref.send_message(admin_tg_id, text_msg)
        except Exception:
            logger.exception("Failed to notify super admin %s for backup", admin_tg_id)


async def _backup_job():
    if not settings.backup_enabled:
        return

    try:
        file_path, table_counts = await _create_db_backup()
        total_rows = sum(table_counts.values())
        summary = ", ".join(f"{name}:{count}" for name, count in sorted(table_counts.items()))
        logger.info("DB backup completed: %s rows=%d", file_path, total_rows)
        await add_stream_event(
            "backup_completed",
            {"file": str(file_path), "rows": total_rows, "tables": table_counts},
        )
        await _notify_backup_status(
            "ðŸ›¡ my_company è‡ªåŠ¨å¤‡ä»½å®Œæˆ\n"
            f"â° åŒ—äº¬æ—¶é—´: {format_bj_now()}\n"
            f"ðŸ“¦ æ–‡ä»¶: {file_path}\n"
            f"ðŸ§¾ æ€»è¡Œæ•°: {total_rows}\n"
            f"ðŸ“š åˆ†è¡¨: {summary}",
        )
    except Exception as exc:
        logger.exception("DB backup failed")
        await add_stream_event("backup_failed", {"error": str(exc)})
        await _notify_backup_status(
            "âŒ my_company è‡ªåŠ¨å¤‡ä»½å¤±è´¥\n"
            f"â° åŒ—äº¬æ—¶é—´: {format_bj_now()}\n"
            f"åŽŸå› : {exc}",
        )


async def _daily_job():
    logger.info("Starting daily settlement...")
    async with async_session() as session:
        async with session.begin():
            reports = await settle_all(session)
    logger.info("Daily settlement completed: %d companies processed", len(reports))
    await add_stream_event("daily_settlement_completed", {"companies": len(reports)})

    # Refresh black market deals for the new day
    try:
        from services.shop_service import generate_black_market
        await generate_black_market()
        logger.info("Black market refreshed")
    except Exception:
        logger.exception("Failed to refresh black market")

    # Notify owners
    if _bot_ref:
        for company, report, events in reports:
            try:
                text = format_daily_report(company, report, events)
                from db.engine import async_session as _sess
                async with _sess() as s:
                    from db.models import User
                    owner = await s.get(User, company.owner_id)
                    if owner:
                        await _bot_ref.send_message(owner.tg_id, text)
            except Exception:
                logger.exception("Failed to notify owner of company %s", company.name)


async def _research_realtime_job():
    """Complete expired research every minute (near-real-time)."""
    from db.models import Company, User
    from services.research_service import check_and_complete_research

    completed_total = 0
    async with async_session() as session:
        async with session.begin():
            companies = (await session.execute(select(Company))).scalars().all()
            for company in companies:
                completed = await check_and_complete_research(session, company.id)
                if not completed:
                    continue
                completed_total += len(completed)
                await add_stream_event(
                    "research_completed",
                    {"company_id": company.id, "company_name": company.name, "techs": completed},
                )
                if _bot_ref:
                    try:
                        owner = await session.get(User, company.owner_id)
                        if not owner:
                            continue
                        await _bot_ref.send_message(
                            owner.tg_id,
                            f"ðŸ”¬ ç§‘ç ”å·²å®Œæˆï¼š{', '.join(completed)}",
                        )
                    except Exception:
                        # é€šçŸ¥å¤±è´¥ä¸å½±å“ç§‘ç ”ç»“ç®—
                        pass
    if completed_total > 0:
        logger.info("Realtime research settlement completed: %d tech(s)", completed_total)


def start_scheduler():
    global _scheduler
    tz = ZoneInfo(settings.app_timezone or "Asia/Shanghai")
    _scheduler = AsyncIOScheduler(timezone=tz)
    _scheduler.add_job(
        _daily_job,
        "cron",
        hour=settings.settlement_hour,
        minute=settings.settlement_minute,
        id="daily_settlement",
    )
    if settings.backup_enabled and settings.backup_interval_minutes > 0:
        _scheduler.add_job(
            _backup_job,
            "cron",
            hour="*/3",
            minute=0,
            id="db_backup",
        )
    _scheduler.add_job(
        _research_realtime_job,
        "interval",
        minutes=1,
        id="research_realtime_settlement",
    )
    _scheduler.start()
    logger.info(
        "Scheduler started: daily settlement at %02d:%02d (%s)",
        settings.settlement_hour,
        settings.settlement_minute,
        settings.app_timezone,
    )
    if settings.backup_enabled and settings.backup_interval_minutes > 0:
        logger.info(
            "Scheduler started: DB backup every 3 hours on the hour (%s, keep %d files)",
            settings.app_timezone,
            settings.backup_keep_files,
        )


def stop_scheduler():
    global _scheduler
    if _scheduler:
        _scheduler.shutdown()
        _scheduler = None
